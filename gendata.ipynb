{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=False,\n",
    "    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]),\n",
    ")\n",
    "\n",
    "testing_data = datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=False,\n",
    "    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "training_data = datasets.CIFAR10(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=False,\n",
    "    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))]),\n",
    ")\n",
    "\n",
    "testing_data = datasets.CIFAR10(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=False,\n",
    "    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "training_data = datasets.CIFAR100(\n",
    "    root=\"./benchmark/cifar100/data/\",\n",
    "    train=True,\n",
    "    download=False,\n",
    "    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))]),\n",
    ")\n",
    "\n",
    "testing_data = datasets.CIFAR100(\n",
    "    root=\"./benchmark/cifar100/data/\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Division algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Each client:**\n",
    "\n",
    "1. Contains no more than 3 labels\n",
    "2. Each label has 8 to 20 samples\n",
    "3. There're at least 5 * #numclass clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncover 0 labels !\n"
     ]
    }
   ],
   "source": [
    "total_labels = np.unique(training_data.targets).tolist()\n",
    "len(total_labels)\n",
    "\n",
    "min_label_per_client = 2\n",
    "max_label_per_client = 5\n",
    "\n",
    "min_sample_per_client = 10\n",
    "max_sample_per_client = 20\n",
    "\n",
    "# num_clients = 5 * len(total_labels)\n",
    "num_clients = 400\n",
    "\n",
    "total_label = len(total_labels)\n",
    "label_list = [i for i in range(total_label)]\n",
    "label_per_client = 2\n",
    "\n",
    "labels = training_data.targets\n",
    "idxs = range(len(training_data))\n",
    "training_idxs_labels = np.vstack((idxs, labels)).T\n",
    "\n",
    "labels = testing_data.targets\n",
    "idxs = range(len(testing_data))\n",
    "testing_idxs_labels = np.vstack((idxs, labels)).T\n",
    "\n",
    "training_dict_client = {client_id:[] for client_id in range(num_clients)}\n",
    "testing_dict_client = {client_id:[] for client_id in range(num_clients)}\n",
    "\n",
    "client_labels = []\n",
    "not_passed_label_list = label_list.copy()\n",
    "\n",
    "for client_id in range(num_clients):\n",
    "    label_per_client = np.random.randint(min_label_per_client, max_label_per_client + 1)\n",
    "    this_set = np.random.choice(label_list, label_per_client, replace=False)\n",
    "    client_labels.append(list(this_set))\n",
    "    not_passed_label_list = list(set(not_passed_label_list) - set(this_set))\n",
    "\n",
    "if len(not_passed_label_list) > 0:\n",
    "    print(\"Uncover\", len(not_passed_label_list), \"labels !\")\n",
    "    exit(0)\n",
    "else:\n",
    "    print(\"Uncover\", len(not_passed_label_list), \"labels !\")\n",
    "\n",
    "samples_details = []\n",
    "\n",
    "for client_idx, client_label in zip(range(num_clients), client_labels):\n",
    "    sample_this_client = []\n",
    "    \n",
    "    for label in client_label:\n",
    "        sample_per_client = np.random.randint(min_sample_per_client, max_sample_per_client + 1)\n",
    "        sample_this_client.append(sample_per_client)\n",
    "        \n",
    "        idxes_1 = training_idxs_labels[training_idxs_labels[:,1] == label][:,0]\n",
    "        idxes_2 = testing_idxs_labels[testing_idxs_labels[:,1] == label][:,0]\n",
    "        \n",
    "        label_1_idxes = np.random.choice(idxes_1, sample_per_client, replace=False)\n",
    "        label_2_idxes = np.random.choice(idxes_2, int(sample_per_client/4), replace=False)\n",
    "        \n",
    "        training_dict_client[client_idx] += label_1_idxes.tolist()\n",
    "        testing_dict_client[client_idx] += label_2_idxes.tolist()\n",
    "        \n",
    "        training_idxs_labels[label_1_idxes] -= 100\n",
    "        testing_idxs_labels[label_2_idxes] -= 100\n",
    "    \n",
    "    samples_details.append(sample_this_client)\n",
    "\n",
    "\n",
    "dis_mtx = np.zeros([num_clients, total_label])\n",
    "for client_id in range(len(client_labels)):\n",
    "    client_label = client_labels[client_id]\n",
    "    client_samples = samples_details[client_id]\n",
    "    \n",
    "    for label, num_samples in zip(client_label, client_samples):\n",
    "        dis_mtx[client_id][label] = num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\" Special json encoder for numpy types \"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "    \n",
    "savepath = f\"./dataset_idx/cifar100/sparse/{num_clients}client\"\n",
    "if not Path(savepath).exists():\n",
    "    os.makedirs(savepath)\n",
    "    \n",
    "json.dump(training_dict_client, open(f\"{savepath}/cifar100_sparse.json\", \"w\"), cls=NumpyEncoder)\n",
    "json.dump(testing_dict_client, open(f\"{savepath}/cifar100_sparse_test.json\", \"w\"), cls=NumpyEncoder)\n",
    "np.savetxt(f\"{savepath}/cifar100_sparse_stat.csv\", dis_mtx, fmt=\"%d\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('longnd')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9f784b053654bb8129a3cb1aa1762d7834caeb9ba8691a85058f59d7796858ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
