{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.MNIST(\n",
    "    root=\"./benchmark/mnist/data/\",\n",
    "    train=True,\n",
    "    download=False,\n",
    "    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]),\n",
    ")\n",
    "\n",
    "testing_data = datasets.MNIST(\n",
    "    root=\"./benchmark/mnist/data/\",\n",
    "    train=False,\n",
    "    download=False,\n",
    "    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.EMNIST(\n",
    "    root=\"./benchmark/emnist/data/\",\n",
    "    split=\"letters\",\n",
    "    train=True,\n",
    "    download=False,\n",
    "    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]),\n",
    ")\n",
    "\n",
    "testing_data = datasets.EMNIST(\n",
    "    root=\"./benchmark/emnist/data/\",\n",
    "    split=\"letters\",\n",
    "    train=False,\n",
    "    download=False,\n",
    "    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./benchmark/cifar10/data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5ebe81cf58545ff931d06bed1677ca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./benchmark/cifar10/data/cifar-10-python.tar.gz to ./benchmark/cifar10/data/\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "training_data = datasets.CIFAR10(\n",
    "    root=\"./benchmark/cifar10/data/\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))]),\n",
    ")\n",
    "\n",
    "testing_data = datasets.CIFAR10(\n",
    "    root=\"./benchmark/cifar10/data/\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./benchmark/fmnist/data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa0bdd5e24e4db680b1a0be48c0f799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26421880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./benchmark/fmnist/data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./benchmark/fmnist/data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./benchmark/fmnist/data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b55019c4e294d9a95ab49eaafd0d733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29515 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./benchmark/fmnist/data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./benchmark/fmnist/data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./benchmark/fmnist/data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edf1683c10dc41138d4c4700d2551346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4422102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./benchmark/fmnist/data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./benchmark/fmnist/data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./benchmark/fmnist/data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11db6a4a646c42fa9d40109edc33ec89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./benchmark/fmnist/data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./benchmark/fmnist/data/FashionMNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"./benchmark/fmnist/data\", \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    ")\n",
    "\n",
    "testing_data = datasets.FashionMNIST(\n",
    "    root=\"./benchmark/fmnist/data\", \n",
    "    train=False, \n",
    "    download=True, \n",
    "    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "training_data = datasets.CIFAR100(\n",
    "    root=\"./benchmark/cifar100/data/\",\n",
    "    train=True,\n",
    "    download=False,\n",
    "    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))]),\n",
    ")\n",
    "\n",
    "testing_data = datasets.CIFAR100(\n",
    "    root=\"./benchmark/cifar100/data/\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Division algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Each client:**\n",
    "\n",
    "1. Contains no more than 3 labels\n",
    "2. Each label has 8 to 20 samples\n",
    "3. There're at least 5 * #numclass clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Uncover 0 labels !\n"
     ]
    }
   ],
   "source": [
    "total_labels = np.unique(training_data.targets).tolist()\n",
    "len(total_labels)\n",
    "print(total_labels)\n",
    "\n",
    "min_label_per_client = 2\n",
    "max_label_per_client = 4\n",
    "\n",
    "min_sample_per_client = 10\n",
    "max_sample_per_client = 20\n",
    "\n",
    "# num_clients = 5 * len(total_labels)\n",
    "num_clients = 500\n",
    "\n",
    "total_label = len(total_labels)\n",
    "label_list = [i for i in total_labels]\n",
    "label_per_client = 2\n",
    "\n",
    "labels = training_data.targets\n",
    "idxs = range(len(training_data))\n",
    "training_idxs_labels = np.vstack((idxs, labels)).T\n",
    "\n",
    "labels = testing_data.targets\n",
    "idxs = range(len(testing_data))\n",
    "testing_idxs_labels = np.vstack((idxs, labels)).T\n",
    "\n",
    "training_dict_client = {client_id:[] for client_id in range(num_clients)}\n",
    "testing_dict_client = {client_id:[] for client_id in range(num_clients)}\n",
    "\n",
    "client_labels = []\n",
    "not_passed_label_list = label_list.copy()\n",
    "\n",
    "for client_id in range(num_clients):\n",
    "    label_per_client = np.random.randint(min_label_per_client, max_label_per_client + 1)\n",
    "    this_set = np.random.choice(label_list, label_per_client, replace=False)\n",
    "    client_labels.append(list(this_set))\n",
    "    not_passed_label_list = list(set(not_passed_label_list) - set(this_set))\n",
    "\n",
    "if len(not_passed_label_list) > 0:\n",
    "    print(\"Uncover\", len(not_passed_label_list), \"labels !\")\n",
    "    exit(0)\n",
    "else:\n",
    "    print(\"Uncover\", len(not_passed_label_list), \"labels !\")\n",
    "\n",
    "samples_details = []\n",
    "\n",
    "for client_idx, client_label in zip(range(num_clients), client_labels):\n",
    "    sample_this_client = []\n",
    "    \n",
    "    for label in client_label:\n",
    "        sample_per_client = np.random.randint(min_sample_per_client, max_sample_per_client + 1)\n",
    "        sample_this_client.append(sample_per_client)\n",
    "        \n",
    "        idxes_1 = training_idxs_labels[training_idxs_labels[:,1] == label][:,0]\n",
    "        idxes_2 = testing_idxs_labels[testing_idxs_labels[:,1] == label][:,0]\n",
    "        \n",
    "        label_1_idxes = np.random.choice(idxes_1, sample_per_client, replace=False)\n",
    "        label_2_idxes = np.random.choice(idxes_2, int(sample_per_client/4), replace=False)\n",
    "        \n",
    "        training_dict_client[client_idx] += label_1_idxes.tolist()\n",
    "        testing_dict_client[client_idx] += label_2_idxes.tolist()\n",
    "        \n",
    "        training_idxs_labels[label_1_idxes] -= 100\n",
    "        testing_idxs_labels[label_2_idxes] -= 100\n",
    "    \n",
    "    samples_details.append(sample_this_client)\n",
    "\n",
    "\n",
    "dis_mtx = np.zeros([num_clients, total_label])\n",
    "for client_id in range(len(client_labels)):\n",
    "    client_label = client_labels[client_id]\n",
    "    client_samples = samples_details[client_id]\n",
    "    \n",
    "    for label, num_samples in zip(client_label, client_samples):\n",
    "        dis_mtx[client_id][total_labels.index(label)] = num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\" Special json encoder for numpy types \"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "    \n",
    "savepath = f\"./dataset_idx/fmnist/sparse/{num_clients}client\"\n",
    "if not Path(savepath).exists():\n",
    "    os.makedirs(savepath)\n",
    "    \n",
    "json.dump(training_dict_client, open(f\"{savepath}/fmnist_sparse.json\", \"w\"), cls=NumpyEncoder)\n",
    "json.dump(testing_dict_client, open(f\"{savepath}/fmnist_sparse_test.json\", \"w\"), cls=NumpyEncoder)\n",
    "np.savetxt(f\"{savepath}/fmnist_sparse_stat.csv\", dis_mtx, fmt=\"%d\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gen I.i.d data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clients = 30\n",
    "sample_per_client = 200\n",
    "\n",
    "training_dict_client = {client_id:[] for client_id in range(num_clients)}\n",
    "testing_dict_client = {client_id:[] for client_id in range(num_clients)}\n",
    "\n",
    "labels = training_data.targets\n",
    "idxs = range(len(training_data))\n",
    "training_idxs_labels = np.vstack((idxs, labels)).T\n",
    "\n",
    "labels = testing_data.targets\n",
    "idxs = range(len(testing_data))\n",
    "testing_idxs_labels = np.vstack((idxs, labels)).T\n",
    "\n",
    "\n",
    "for client_id in range(num_clients):    \n",
    "    idxes_1 = training_idxs_labels[:,0]\n",
    "    idxes_2 = testing_idxs_labels[:,0]\n",
    "    \n",
    "    label_1_idxes = np.random.choice(idxes_1, sample_per_client, replace=False)\n",
    "    label_2_idxes = np.random.choice(idxes_2, int(sample_per_client/4), replace=False)\n",
    "    \n",
    "    training_dict_client[client_id] += label_1_idxes.tolist()\n",
    "    testing_dict_client[client_id] += label_2_idxes.tolist()\n",
    "    \n",
    "    training_idxs_labels[label_1_idxes] -= 100\n",
    "    testing_idxs_labels[label_2_idxes] -= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\" Special json encoder for numpy types \"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "    \n",
    "savepath = f\"./dataset_idx/mnist/iid/{num_clients}client\"\n",
    "if not Path(savepath).exists():\n",
    "    os.makedirs(savepath)\n",
    "    \n",
    "json.dump(training_dict_client, open(f\"{savepath}/mnist_iid.json\", \"w\"), cls=NumpyEncoder)\n",
    "json.dump(testing_dict_client, open(f\"{savepath}/mnist_iid_test.json\", \"w\"), cls=NumpyEncoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('longnd')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9f784b053654bb8129a3cb1aa1762d7834caeb9ba8691a85058f59d7796858ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
